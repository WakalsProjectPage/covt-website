<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>CoVT — Project Page</title>
    <meta name="description" content="AI Paper Project Page" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="static/css/styles.css" />
  </head>
  <body>
    <!-- 侧边导航触发器（移动端/窄屏） -->
    <button class="sidebar-toggle" id="sidebarToggle" aria-label="切换目录" aria-expanded="true">
      <span class="arrow" aria-hidden="true"></span>
    </button>

    <!-- 侧边导航 -->
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-inner glass">
        <div class="brand">CoVT</div>
        <nav class="toc">
          <a href="#hero" data-ease>Teaser</a>
          <a href="#abstract" data-ease>Abstract</a>
          <a href="#method" data-ease>Method</a>
          <!-- <a href="#gallery-single" data-ease>Gallery · Single</a> -->
          <a href="#anchor_visualization" data-ease>Anchor Token Visualization</a>
          <a href="#experiment-results" data-ease>Experiment Results</a>
        </nav>
      </div>
    </aside>

    <!-- 回到顶部按钮 -->
    <button id="backToTop" class="back-to-top" aria-label="Back to Top">⌃</button>

    <main>
      <!-- 首屏 Teaser 区块 -->
      <section id="hero" class="section hero">
        <div class="hero-bg" aria-hidden="true"></div>
        <div class="hero-inner">
          <h1 class="title">Chain-of-Visual-Thought: Teaching VLMs to See and Think Better with Continuous Visual Tokens</h1>
          <!-- <p class="subtitle">A framework that enables VLMs to reason not only in words but also through continuous visual tokens—compact latent representations that encode rich perceptual cues. Within a small budget of roughly 20 tokens, CoVT distills knowledge from lightweight vision experts capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure.</p> -->

          <div class="meta">
            <!-- <div class="authors"></div>
            <div class="affiliations"></div> -->
          </div>

          <div class="cta">
            <a class="btn primary" href="#" target="_blank" rel="noopener">Paper</a>
            <a class="btn" href="#" target="_blank" rel="noopener">arXiv</a>
            <a class="btn" href="https://github.com/Wakals/CoMT" target="_blank" rel="noopener">Code</a>
            <a class="btn" href="#" target="_blank" rel="noopener">HF Model</a>
            <a class="btn" href="#" target="_blank" rel="noopener">HF Data</a>
          </div>
        </div>
        
        <figure style="background:transparent; box-shadow:none; border:none; margin:0; padding:0; display:flex; justify-content:center;">
          <img src="static/image/teaser.png" alt="Teaser Image" style="width:65%; margin:0; display:block; border:0; box-shadow:none; background:transparent;" />
        </figure>
      </section>

      <!-- Abstract -->
      <section id="abstract" class="section container reveal">
        <h2>Abstract</h2>
        <p>
          Vision–Language Models (VLMs) excel at reasoning in linguistic space but struggle with perceptual understanding that requires dense visual perception, e.g., spatial reasoning and geometric awareness.
          This limitation stems from the fact that current VLMs have limited mechanisms to capture dense visual information across spatial dimensions.We introduce Chain-of-Visual-Thought (CoVT), a framework that enables VLMs to reason not only in words but also through continuous visual tokens—compact latent representations that encode rich perceptual cues.
          Within a small budget of roughly 20 tokens, CoVT distills knowledge from lightweight vision experts capturing complementary properties such as 2D appearance, 3D geometry, spatial layout, and edge structure.
          During training, the VLM with CoVTautoregressively predicts these visual tokens to reconstruct dense supervision signals (e.g., depth, segmentation, edges, and DINO features).At inference, the model reasons directly in the continuous visual token space, preserving efficiency while optionally decoding dense predictions for interpretability.
          Evaluated across more than ten diverse perception benchmarks, including CV-Bench, MMVP, RealWorldQA, MMStar, WorldMedQA, and HRBench, integrating CoVT into strong VLMs such as Qwen2.5-VL and LLaVA consistently improves performance by 3% to 16% and demonstrates that compact continuous visual thinking enables more precise, grounded, and interpretable multimodal intelligence.
        </p>
      </section>

      <!-- Method -->
      <section id="method" class="section container reveal">
        <h2>Method</h2>
        <p>
          CoMT generates visual anchor tokens and then leverages these latents to condition next token prediction and reason the final answer. 
          This is achieved by aligning the anchor tokens with "expert" model features on their corresponding tasks during training. 
          Four kinds of anchor tasks are introduced.
        </p>
        <div class="card-grid">
          <div class="card glass">
            <h3>SAM</h3>
            <p>SAM is for segmentation task and provides instance locations and shapes information of VLMs.</p>
          </div>
          <div class="card glass">
            <h3>DepthAnything</h3>
            <p>DepthAnything is for the depth estimation task and provides 3D perception in real-world problems.</p>
          </div>
          <div class="card glass">
            <h3>PIDINet</h3>
            <p>PIDINet is employed to extract the edge information of an image, thereby providing cues about the spatial locations of certain objects.</p>
          </div>
          <div class="card glass">
            <h3>DINO</h3>
            <p>DINO is a visual representation model for extracting image features and provides feature perception for VLMs.</p>
          </div>
        </div>
      </section>

      <!-- 单图画廊模板 -->
      <!-- <section id="gallery-single" class="section container reveal">
        <div class="section-head">
          <h2>Gallery · Single</h2>
          <p>左右箭头切换，点击图片展示对应 caption，底部进度条反映位置。</p>
        </div>
        <div class="carousel glass" data-carousel>
          <button class="nav prev" data-prev aria-label="上一张">‹</button>
          <div class="viewport">
            <div class="track" id="carouselTrack">
            </div>
          </div>
          <button class="nav next" data-next aria-label="下一张">›</button>
          <div class="progress"><div class="bar" id="carouselProgress"></div></div>
          <div class="caption" id="carouselCaption">点击图片显示说明</div>
        </div>
      </section> -->

      <section id="anchor_visualization" class="section container reveal">
        <div class="section-head">
          <h2>Anchor Token Demo Visualization</h2>
          <p>Different visual tokens contribute complementary cues that enable the model to solve complex perceptual reasoning tasks. Here we provide a few demo visualizations for each anchor token type.</p>
        </div>

        <div class="variant-picker" id="variantPicker" role="tablist" aria-label="选择编辑版本">
          <button role="tab" aria-selected="true" data-variant="v1" class="chip">demo1</button>
          <button role="tab" aria-selected="false" data-variant="v2" class="chip">demo2</button>
          <button role="tab" aria-selected="false" data-variant="v3" class="chip">demo3</button>
        </div>

        <div class="vc-panels" id="vcPanels"></div>
        <div class="vc-panel" data-variant="v1">


          <div class="variant-row" style="display:flex;gap:16px;align-items:flex-start;flex-wrap:wrap;">
            <div style="flex:0 0 28%;min-width:200px;">
              <img src="static/image/demo1.png" alt="demo1" style="width:100%;display:block;margin:0;" />
            </div>
            <div style="flex:1 1 68%;min-width:200px;display:flex;align-items:center;">
              <div style="display:flex;flex-direction:column;gap:12px;width:100%;">
          
          <!-- Question block -->
          <div class="qa-block question-block">
            <div class="qa-label qa-label-question">Question</div>
            <p class="qa-text">
              Two points are circled on the image, labeled by A and B beside each circle.
              Which point is closer to the camera?
            </p>
          </div>

          <!-- Answer block -->
          <div class="qa-block answer-block">
            <div class="qa-label qa-label-answer">Answer</div>
            <div>
              <p class="qa-text">
                <code class="tag tag-think">&lt;Think&gt;</code>
                Because the segmentation of the image is
                <code class="token token-sam">&lt;SAM Token&gt;</code>
                and the depth map of the image is
                <code class="token token-depth">&lt;DEPTH Token&gt;</code>
                <code class="tag tag-think">&lt;/Think&gt;</code>
              </p>
              <p class="qa-text qa-text-strong">
                <code class="tag tag-answer">&lt;Answer&gt;</code>
                The answer is the point B.
                <code class="tag tag-answer">&lt;/Answer&gt;</code>
              </p>
            </div>
          </div>
              </div>
            </div>
          </div>
        </div>

        <div class="vc-panel" data-variant="v2" hidden>
          <div class="variant-row" style="display:flex;gap:16px;align-items:flex-start;flex-wrap:wrap;">
            <div style="flex:0 0 28%;min-width:200px;">
              <img src="static/image/demo2.png" alt="demo1" style="width:100%;display:block;margin:0;" />
            </div>
            <div style="flex:1 1 68%;min-width:200px;display:flex;align-items:center;">
              <div style="display:flex;flex-direction:column;gap:12px;width:100%;">
                
                <!-- Question block -->
                <div class="qa-block question-block">
                  <div class="qa-label qa-label-question">Question</div>
                  <p class="qa-text">
                    Which object is closer to the camera taking this photo,
                    the books (highlighted by a red box) or the chair (highlighted by a blue box)? 
                  </p>
                </div>

                <!-- Answer block -->
                <div class="qa-block answer-block">
                  <div class="qa-label qa-label-answer">Answer</div>
                  <div>
                    <p class="qa-text">
                      <code class="tag tag-think">&lt;Think&gt;</code>
                      Because the segmentation of the image is
                      <code class="token token-depth">&lt;DEPTH Token&gt;</code>
                      and the depth map of the image is
                      <code class="token token-edge">&lt;Edge Token&gt;</code>
                      <code class="tag tag-think">&lt;/Think&gt;</code>
                    </p>
                    <p class="qa-text qa-text-strong">
                      <code class="tag tag-answer">&lt;Answer&gt;</code>
                      The <strong>books</strong> (highlighted by a red box) are closer to the camera than the chair (highlighted by a blue box).
                      <code class="tag tag-answer">&lt;/Answer&gt;</code>
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="vc-panel" data-variant="v3" hidden>
          <div class="variant-row" style="display:flex;gap:16px;align-items:flex-start;flex-wrap:wrap;">
            <div style="flex:0 0 28%;min-width:200px;">
              <img src="static/image/demo3.png" alt="demo1" style="width:100%;display:block;margin:0;" />
            </div>
            <div style="flex:1 1 68%;min-width:200px;display:flex;align-items:center;">
              <div style="display:flex;flex-direction:column;gap:12px;width:100%;">
                
                <!-- Question block -->
                <div class="qa-block question-block">
                  <div class="qa-label qa-label-question">Question</div>
                  <p class="qa-text">
                    How many white vertical lines are drawn on the ground?
                  </p>
                </div>

                <!-- Answer block -->
                <div class="qa-block answer-block">
                  <div class="qa-label qa-label-answer">Answer</div>
                  <div>
                    <p class="qa-text">
                      <code class="tag tag-think">&lt;Think&gt;</code>
                      Because the segmentation of the image is
                      <code class="token token-depth">&lt;DEPTH Token&gt;</code>
                      and the depth map of the image is
                      <code class="token token-edge">&lt;Edge Token&gt;</code>
                      <code class="tag tag-think">&lt;/Think&gt;</code>
                    </p>
                    <p class="qa-text qa-text-strong">
                      <code class="tag tag-answer">&lt;Answer&gt;</code>
                      There are <strong>five</strong> vertical lines in the image.
                      <code class="tag tag-answer">&lt;/Answer&gt;</code>
                    </p>
                  </div>
                </div>
              </div>
            </div>
          </div>
        </div>

        <div class="compare-grid">
          <div class="compare glass">
            <div class="img original">
              <img class="js-original" alt="原图" />
              <span class="tag">Original</span>
            </div>
            <div class="img edited">
              <img class="js-editedA" alt="编辑图" />
              <span class="tag js-visualTokenTagA">Visual Token</span>
              <script>
              (function(){
                const tagEl = document.querySelector('.js-visualTokenTagA');
                const picker = document.getElementById('variantPicker');
                if (!tagEl || !picker) return;
                const map = { v1: 'SAM Token', v2: 'Depth Token', v3: 'SAM Token' };
                function update(variant){
                  tagEl.textContent = map[variant] || 'Visual Token';
                }
                picker.addEventListener('click', (e) => {
                  const btn = e.target.closest('[data-variant]');
                  if (btn) update(btn.getAttribute('data-variant'));
                });
                const init = picker.querySelector('[aria-selected="true"]')?.getAttribute('data-variant');
                if (init) update(init);
              })();
              </script>
            </div>
            <input type="range" class="js-slider" min="3" max="97" value="50" aria-label="对比滑块" />
          </div>
          <div class="compare glass">
            <div class="img original">
              <img class="js-original" alt="原图" />
              <span class="tag">Original</span>
            </div>
            <div class="img edited">
              <img class="js-editedB" alt="编辑图" />
              <span class="tag js-visualTokenTagB">Visual Token</span>
              <script>
              (function(){
                const tagEl = document.querySelector('.js-visualTokenTagB');
                const picker = document.getElementById('variantPicker');
                if (!tagEl || !picker) return;
                const map = { v1: 'Depth Token', v2: 'Edge Token', v3: 'Edge Token' };
                function update(variant){
                  tagEl.textContent = map[variant] || 'Visual Token';
                }
                picker.addEventListener('click', (e) => {
                  const btn = e.target.closest('[data-variant]');
                  if (btn) update(btn.getAttribute('data-variant'));
                });
                const init = picker.querySelector('[aria-selected="true"]')?.getAttribute('data-variant');
                if (init) update(init);
              })();
              </script>
            </div>
            <input type="range" class="js-slider" min="3" max="97" value="50" aria-label="对比滑块" />
          </div>
        </div>
      </section>

      <!-- Experiments -->
      <section id="experiment-results" class="section container reveal">
        <h2>Experiment Results</h2>
        <p> Placeholder for experiment results images </p>
        <img src="static/image/result_major_bench.png" alt="Result Table" style="width: 100%; display: block; margin: 0 auto;" />
      </section>

      <!-- 视频模块 -->
      <!-- <section id="video" class="section container reveal">
        <h2>Video</h2>
        <div class="video-grid">
          <video class="glass" controls preload="metadata" poster="" id="localVideo">
             用户可替换 source 为真实视频文件 
          </video>
          <div class="glass iframe-wrap">
            <iframe
              id="ytEmbed"
              title="YouTube Demo"
              src="https://www.youtube.com/embed/dQw4w9WgXcQ"
              loading="lazy"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </section> -->

      <footer class="footer container">
        <p>2025 CoMT. Crafted with ❤️ in teal & light-blue.</p>
      </footer>
    </main>

    <script src="static/js/script.js"></script>
  </body>
  </html>


