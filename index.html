<!DOCTYPE html>
<html lang="zh-CN">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <title>CoMT — Project Page</title>
    <meta name="description" content="AI Paper Project Page" />
    <link rel="preconnect" href="https://fonts.googleapis.com" />
    <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin />
    <link href="https://fonts.googleapis.com/css2?family=Inter:wght@300;400;500;600;700;800;900&display=swap" rel="stylesheet" />
    <link rel="stylesheet" href="static/css/styles.css" />
  </head>
  <body>
    <!-- 侧边导航触发器（移动端/窄屏） -->
    <button class="sidebar-toggle" id="sidebarToggle" aria-label="切换目录" aria-expanded="true">
      <span class="arrow" aria-hidden="true"></span>
    </button>

    <!-- 侧边导航 -->
    <aside class="sidebar" id="sidebar">
      <div class="sidebar-inner glass">
        <div class="brand">CoMT</div>
        <nav class="toc">
          <a href="#hero" data-ease>Teaser</a>
          <a href="#abstract" data-ease>Abstract</a>
          <a href="#method" data-ease>Method</a>
          <!-- <a href="#gallery-single" data-ease>Gallery · Single</a> -->
          <a href="#anchor_visualization" data-ease>Anchor Token Visualization</a>
          <a href="#experiment-results" data-ease>Experiment Results</a>
        </nav>
      </div>
    </aside>

    <!-- 回到顶部按钮 -->
    <button id="backToTop" class="back-to-top" aria-label="Back to Top">⌃</button>

    <main>
      <!-- 首屏 Teaser 区块 -->
      <section id="hero" class="section hero">
        <div class="hero-bg" aria-hidden="true"></div>
        <div class="hero-inner">
          <h1 class="title">CoMT: Chain-of-Multimodal-Thought</h1>
          <p class="subtitle">Trains VLMs to generate anchored tokens jointly with text, enabling them to internalize and reason over visual cues directly in token space.</p>

          <div class="meta">
            <!-- <div class="authors"></div>
            <div class="affiliations"></div> -->
          </div>

          <div class="cta">
            <a class="btn primary" href="#" target="_blank" rel="noopener">Paper</a>
            <a class="btn" href="#" target="_blank" rel="noopener">arXiv</a>
            <a class="btn" href="https://github.com/Wakals/CoMT" target="_blank" rel="noopener">Code</a>
            <a class="btn" href="#" target="_blank" rel="noopener">HF Model</a>
            <a class="btn" href="#" target="_blank" rel="noopener">HF Data</a>
          </div>
        </div>
        
        <figure class="carousel" aria-hidden="true">
          <img src="static/image/teaser.png" alt="Teaser Image" style="width: 100%; display: block; margin: 0 auto;" />
        </figure>
      </section>

      <!-- Abstract -->
      <section id="abstract" class="section container reveal">
        <h2>Abstract</h2>
        <p>
          Vision–Language Models (VLMs) excel at language reasoning but falter on advanced fine-grained visual tasks. Lacking sophisticated perceptual abilities, they can miss spatial cues and struggle to represent rich visual with discrete tokens.
          To address these limitations, we introduce CoMT, a framework that teaches VLMs to both see and think by interleaving discrete text tokens with continuous visual tokens—basic perceptual signals such as segmentation, depth, and edges that ground models in visual space. These tokens act as "anchors," linking language reasoning to core visual capabilities and directing attention to fine-grained details. CoMT trains VLMs to generate anchored tokens jointly with text, enabling them to internalize and reason over visual cues directly in token space. At inference time, the model uses these tokens to ground its outputs without needing explicit maps or external tool calls.
          Unlike prior tool-using VLMs, which invoke APIs and parse discrete outputs such as bounding boxes or depth maps—often requiring extra control logic or reinforcement learning—CoMT integrates perceptual signals natively into the token space. This yields efficient, self-contained, and visually grounded reasoning.
          Extensive evaluations show that CoMT consistently improves performance on visual reasoning tasks such as counting, localization, depth, correspondence, and similarity, achieving strong gains on CV-Bench, BLINK, RealWorldQA, and MMStar.
        </p>
      </section>

      <!-- Method -->
      <section id="method" class="section container reveal">
        <h2>Method</h2>
        <p>
          CoMT generates visual anchor tokens and then leverages these latents to condition next token prediction and reason the final answer. 
          This is achieved by aligning the anchor tokens with "expert" model features on their corresponding tasks during training. 
          Four kinds of anchor tasks are introduced.
        </p>
        <div class="card-grid">
          <div class="card glass">
            <h3>SAM</h3>
            <p>SAM is for segmentation task and provides instance locations and shapes information of VLMs.</p>
          </div>
          <div class="card glass">
            <h3>DepthAnything</h3>
            <p>DepthAnything is for the depth estimation task and provides 3D perception in real-world problems.</p>
          </div>
          <div class="card glass">
            <h3>PIDINet</h3>
            <p>PIDINet is employed to extract the edge information of an image, thereby providing cues about the spatial locations of certain objects.</p>
          </div>
          <div class="card glass">
            <h3>DINO</h3>
            <p>DINO is a visual representation model for extracting image features and provides feature perception for VLMs.</p>
          </div>
        </div>
      </section>

      <!-- 单图画廊模板 -->
      <!-- <section id="gallery-single" class="section container reveal">
        <div class="section-head">
          <h2>Gallery · Single</h2>
          <p>左右箭头切换，点击图片展示对应 caption，底部进度条反映位置。</p>
        </div>
        <div class="carousel glass" data-carousel>
          <button class="nav prev" data-prev aria-label="上一张">‹</button>
          <div class="viewport">
            <div class="track" id="carouselTrack">
            </div>
          </div>
          <button class="nav next" data-next aria-label="下一张">›</button>
          <div class="progress"><div class="bar" id="carouselProgress"></div></div>
          <div class="caption" id="carouselCaption">点击图片显示说明</div>
        </div>
      </section> -->

      <section id="anchor_visualization" class="section container reveal">
        <div class="section-head">
          <h2>Anchor Token Visualization</h2>
          <p>选择版本后，通过滑块在原图与编辑图之间拖动分割线。</p>
        </div>
        <div class="variant-picker" id="variantPicker" role="tablist" aria-label="选择编辑版本">
          <button role="tab" aria-selected="true" data-variant="v1" class="chip">sam</button>
          <button role="tab" aria-selected="false" data-variant="v2" class="chip">depth</button>
          <button role="tab" aria-selected="false" data-variant="v3" class="chip">pidinet</button>
        </div>

        <div class="compare-grid">
          <div class="compare glass">
            <div class="img original">
              <img class="js-original" alt="原图" />
              <span class="tag">Original</span>
            </div>
            <div class="img edited">
              <img class="js-edited" alt="编辑图" />
              <span class="tag">Visual Token</span>
            </div>
            <input type="range" class="js-slider" min="0" max="100" value="50" aria-label="对比滑块" />
          </div>
          <div class="compare glass">
            <div class="img original">
              <img class="js-original" alt="原图" />
              <span class="tag">Original</span>
            </div>
            <div class="img edited">
              <img class="js-edited" alt="编辑图" />
              <span class="tag">Visual Token</span>
            </div>
            <input type="range" class="js-slider" min="0" max="100" value="50" aria-label="对比滑块" />
          </div>
        </div>
      </section>

      <!-- Experiments -->
      <section id="experiment-results" class="section container reveal">
        <h2>Experiment Results</h2>
        <p> Placeholder for experiment results images </p>
        <img src="static/image/result_major_bench.png" alt="Result Table" style="width: 100%; display: block; margin: 0 auto;" />
      </section>

      <!-- 视频模块 -->
      <!-- <section id="video" class="section container reveal">
        <h2>Video</h2>
        <div class="video-grid">
          <video class="glass" controls preload="metadata" poster="" id="localVideo">
             用户可替换 source 为真实视频文件 
          </video>
          <div class="glass iframe-wrap">
            <iframe
              id="ytEmbed"
              title="YouTube Demo"
              src="https://www.youtube.com/embed/dQw4w9WgXcQ"
              loading="lazy"
              allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture; web-share"
              allowfullscreen
            ></iframe>
          </div>
        </div>
      </section> -->

      <footer class="footer container">
        <p>2025 CoMT. Crafted with ❤️ in teal & light-blue.</p>
      </footer>
    </main>

    <script src="static/js/script.js"></script>
  </body>
  </html>


